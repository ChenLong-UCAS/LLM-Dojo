{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-19T09:58:03.864197500Z",
     "start_time": "2024-04-19T09:58:03.848575200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank,  alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(torch.rand(in_dim, rank)*std_dev)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features,\n",
    "            linear.out_features,\n",
    "            rank,\n",
    "            alpha\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.linear(x) + self.lora(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-19T09:58:04.834537Z",
     "start_time": "2024-04-19T09:58:04.814072800Z"
    }
   },
   "id": "5043ad65d0e71bce"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original output: tensor([[0.6639, 0.4487]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "layer = nn.Linear(10, 2)\n",
    "x = torch.randn((1, 10))\n",
    "\n",
    "print(\"Original output:\", layer(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-19T09:58:55.013760300Z",
     "start_time": "2024-04-19T09:58:54.424412100Z"
    }
   },
   "id": "ed917b2c22108634"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA output: tensor([[0.6639, 0.4487]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_lora_1 = LinearWithLoRA(layer, rank=2, alpha=4)\n",
    "print(\"LoRA output:\", layer_lora_1(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-19T10:00:32.513380100Z",
     "start_time": "2024-04-19T10:00:32.325894100Z"
    }
   },
   "id": "7d88cf835c6a2479"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "class TestMLP(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden1, num_hidden2, num_class):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden1, num_hidden2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(num_hidden2, num_class)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-19T12:07:26.685145600Z",
     "start_time": "2024-04-19T12:07:26.660941500Z"
    }
   },
   "id": "32464af84070e2ec"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestMLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_features = 64\n",
    "num_hidden1 = 32\n",
    "num_hidden2=64\n",
    "num_class=3\n",
    "\n",
    "model = TestMLP(\n",
    "    num_features=num_features,\n",
    "    num_hidden1=num_hidden1,\n",
    "    num_hidden2=num_hidden2,\n",
    "    num_class=num_class\n",
    ")\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-19T12:07:27.941053600Z",
     "start_time": "2024-04-19T12:07:27.923061800Z"
    }
   },
   "id": "2713442117d0ea52"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "model.layers[0] = LinearWithLoRA(model.layers[0], rank=4, alpha=8)\n",
    "model.layers[2] = LinearWithLoRA(model.layers[2], rank=4, alpha=8)\n",
    "model.layers[4] = LinearWithLoRA(model.layers[4], rank=4, alpha=8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-19T12:07:29.649960200Z",
     "start_time": "2024-04-19T12:07:29.633962500Z"
    }
   },
   "id": "f2112128ba1b8f56"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestMLP(\n",
      "  (layers): Sequential(\n",
      "    (0): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=32, out_features=64, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=64, out_features=3, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-19T11:11:45.853526500Z",
     "start_time": "2024-04-19T11:11:45.841496500Z"
    }
   },
   "id": "13fd3d2e85230f4c"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearWithLoRA(\n",
      "  (linear): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (lora): LoRALayer()\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[-0.1158, -0.1105, -0.0548,  ..., -0.0058,  0.0012,  0.0280],\n",
      "        [ 0.0602,  0.0445,  0.0696,  ..., -0.0741,  0.0580,  0.0289],\n",
      "        [-0.0630, -0.1075, -0.0224,  ..., -0.1188, -0.0248,  0.1149],\n",
      "        ...,\n",
      "        [ 0.1098, -0.0895, -0.0360,  ..., -0.0164,  0.0301,  0.0624],\n",
      "        [ 0.0312, -0.0655, -0.0765,  ..., -0.0780, -0.1172,  0.0231],\n",
      "        [-0.0661, -0.0488,  0.0720,  ...,  0.0444,  0.0489, -0.0636]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0066, -0.1249, -0.0943, -0.0533, -0.0540, -0.0756,  0.0510,  0.0071,\n",
      "         0.0478, -0.0202,  0.0831, -0.0763,  0.0425, -0.0740, -0.0455,  0.1156,\n",
      "        -0.0246,  0.0130,  0.0421,  0.0892,  0.0456, -0.0929, -0.0974, -0.0031,\n",
      "        -0.0114,  0.0788, -0.0324,  0.1228,  0.1034,  0.1236,  0.0059, -0.0088],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.4334, 0.4897, 0.3964, 0.4209],\n",
      "        [0.2370, 0.2048, 0.2253, 0.3505],\n",
      "        [0.0957, 0.3652, 0.0881, 0.0241],\n",
      "        [0.1565, 0.1682, 0.3388, 0.4553],\n",
      "        [0.4748, 0.0303, 0.3059, 0.2028],\n",
      "        [0.4224, 0.1620, 0.2037, 0.0476],\n",
      "        [0.2620, 0.4821, 0.4002, 0.3520],\n",
      "        [0.2487, 0.0111, 0.2566, 0.3457],\n",
      "        [0.0119, 0.0751, 0.2686, 0.3936],\n",
      "        [0.1144, 0.0181, 0.0701, 0.3836],\n",
      "        [0.4452, 0.0411, 0.0555, 0.2285],\n",
      "        [0.4668, 0.1868, 0.4207, 0.3513],\n",
      "        [0.2675, 0.0355, 0.4395, 0.2057],\n",
      "        [0.3091, 0.1154, 0.1357, 0.1111],\n",
      "        [0.0061, 0.2299, 0.4837, 0.0117],\n",
      "        [0.1918, 0.0050, 0.1254, 0.2404],\n",
      "        [0.0757, 0.3808, 0.1806, 0.1601],\n",
      "        [0.0851, 0.3748, 0.4526, 0.4688],\n",
      "        [0.0396, 0.0127, 0.1507, 0.3144],\n",
      "        [0.2683, 0.3663, 0.2356, 0.3810],\n",
      "        [0.1031, 0.1477, 0.2407, 0.4910],\n",
      "        [0.1846, 0.3516, 0.3309, 0.1656],\n",
      "        [0.0750, 0.3548, 0.3858, 0.0464],\n",
      "        [0.1413, 0.1424, 0.4450, 0.0319],\n",
      "        [0.2991, 0.1826, 0.0859, 0.4951],\n",
      "        [0.1125, 0.1026, 0.1186, 0.3802],\n",
      "        [0.2416, 0.1150, 0.1342, 0.0384],\n",
      "        [0.4476, 0.2267, 0.1713, 0.3109],\n",
      "        [0.1685, 0.2884, 0.4430, 0.3916],\n",
      "        [0.4892, 0.2555, 0.1523, 0.0573],\n",
      "        [0.4023, 0.2419, 0.2643, 0.4085],\n",
      "        [0.1607, 0.2422, 0.3978, 0.1473],\n",
      "        [0.0088, 0.3671, 0.1788, 0.1176],\n",
      "        [0.2525, 0.4506, 0.2302, 0.4950],\n",
      "        [0.1948, 0.3419, 0.3121, 0.1837],\n",
      "        [0.1223, 0.3150, 0.1805, 0.1201],\n",
      "        [0.2501, 0.4769, 0.1064, 0.4267],\n",
      "        [0.2080, 0.4725, 0.1363, 0.1490],\n",
      "        [0.4837, 0.2107, 0.4866, 0.1918],\n",
      "        [0.0068, 0.4115, 0.3387, 0.2163],\n",
      "        [0.2661, 0.4287, 0.0948, 0.3048],\n",
      "        [0.0479, 0.0347, 0.1714, 0.0480],\n",
      "        [0.0071, 0.0590, 0.4627, 0.4296],\n",
      "        [0.1909, 0.4545, 0.3115, 0.0970],\n",
      "        [0.4196, 0.2969, 0.4907, 0.4594],\n",
      "        [0.1535, 0.2913, 0.0197, 0.4667],\n",
      "        [0.1258, 0.2024, 0.1499, 0.4943],\n",
      "        [0.2619, 0.0708, 0.0235, 0.4626],\n",
      "        [0.3794, 0.3323, 0.0524, 0.2836],\n",
      "        [0.4675, 0.3457, 0.2612, 0.2958],\n",
      "        [0.1167, 0.1091, 0.0818, 0.0366],\n",
      "        [0.0402, 0.3757, 0.3729, 0.3161],\n",
      "        [0.3673, 0.0194, 0.2163, 0.0043],\n",
      "        [0.3806, 0.0436, 0.2191, 0.1923],\n",
      "        [0.2985, 0.4702, 0.1768, 0.4265],\n",
      "        [0.2634, 0.1051, 0.2627, 0.0740],\n",
      "        [0.2004, 0.4624, 0.0220, 0.1133],\n",
      "        [0.3226, 0.0847, 0.1634, 0.4288],\n",
      "        [0.4501, 0.1120, 0.0618, 0.3098],\n",
      "        [0.0089, 0.2377, 0.1575, 0.4212],\n",
      "        [0.4193, 0.0897, 0.0436, 0.1375],\n",
      "        [0.3253, 0.4620, 0.3806, 0.1725],\n",
      "        [0.4472, 0.3955, 0.0591, 0.2616],\n",
      "        [0.3882, 0.4746, 0.3453, 0.0947]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for child in model.children():\n",
    "    for c in child.children():\n",
    "        print(c)\n",
    "        for i , j in c.named_parameters():\n",
    "            print(j)\n",
    "        break\n",
    "        if isinstance(c, LinearWithLoRA):\n",
    "            print(\"this is lora\")\n",
    "        print('---------')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-19T11:43:08.879737300Z",
     "start_time": "2024-04-19T11:43:08.851846900Z"
    }
   },
   "id": "8e4ed896ccabf110"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.linear.weight:False\n",
      "layers.0.linear.bias:False\n",
      "layers.0.lora.A:True\n",
      "layers.0.lora.B:True\n",
      "layers.2.linear.weight:False\n",
      "layers.2.linear.bias:False\n",
      "layers.2.lora.A:True\n",
      "layers.2.lora.B:True\n",
      "layers.4.linear.weight:False\n",
      "layers.4.linear.bias:False\n",
      "layers.4.lora.A:True\n",
      "layers.4.lora.B:True\n"
     ]
    }
   ],
   "source": [
    "def freeze_linear_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            # Recursively freeze linear layers in children modules\n",
    "            freeze_linear_layers(child)\n",
    "\n",
    "freeze_linear_layers(model)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'{name}:{param.requires_grad}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-19T12:07:32.894216400Z",
     "start_time": "2024-04-19T12:07:32.866217500Z"
    }
   },
   "id": "314d254f8fe625bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
