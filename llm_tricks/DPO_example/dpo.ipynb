{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 从0实现一个DPO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb69f628966bb719"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.准备数据"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bbb0f857b2a051"
  },
  {
   "cell_type": "markdown",
   "source": [
    "DPO所需要的数据主要三个字段：\n",
    "- instruction：指令问题\n",
    "- chosen：选择的偏好回答\n",
    "- rejected: 不好的回答"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76726cb0c67792af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"./unsloth_dpo.jsonl\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data_list = file.readlines()\n",
    "# data = json.loads(data_list)\n",
    "data = [json.loads(data) for data in data_list]\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "691f55cb1faa5a5f"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{'prompt': 'What is one benefit of using Unsloth for LLM fine-tuning?',\n 'chosen': 'One benefit of using Unsloth for LLM fine-tuning is that it offers a 0% accuracy degradation compared to normal QLoRA, as no approximations are made in the optimized code.',\n 'rejected': 'Using Unsloth for LLM fine-tuning increases accuracy degradation compared to normal QLoRA.'}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-14T09:17:46.958167500Z",
     "start_time": "2024-08-14T09:17:46.942167Z"
    }
   },
   "id": "9aa37c970611f41d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'prompt': 'How can Unsloth accelerate LLM fine-tuning?',\n  'chosen': 'Unsloth accelerates LLM fine-tuning by overwriting some parts of the modeling code with optimized operations and rewriting all Pytorch modules into Triton kernels, resulting in a 2x speedup and a 40% reduction in memory usage.',\n  'rejected': 'Unsloth slows down LLM fine-tuning.'},\n {'prompt': 'What is one benefit of using Unsloth for LLM fine-tuning?',\n  'chosen': 'One benefit of using Unsloth for LLM fine-tuning is that it offers a 0% accuracy degradation compared to normal QLoRA, as no approximations are made in the optimized code.',\n  'rejected': 'Using Unsloth for LLM fine-tuning increases accuracy degradation compared to normal QLoRA.'},\n {'prompt': 'Who developed the Unsloth library for LLM fine-tuning?',\n  'chosen': 'The Unsloth library for LLM fine-tuning was developed by the Unsloth team, including Daniel and Michael, in collaboration with the open-source community.',\n  'rejected': 'The Unsloth library for LLM fine-tuning was developed by Google and Microsoft.'},\n {'prompt': 'What GPU types are compatible with Unsloth for LLM fine-tuning?',\n  'chosen': 'Unsloth supports most NVIDIA GPUs, ranging from GTX 1070 to H100s, making it compatible with a wide range of hardware configurations.',\n  'rejected': 'Unsloth only supports AMD GPUs for LLM fine-tuning.'},\n {'prompt': 'How does Unsloth contribute to reducing memory usage during LLM fine-tuning?',\n  'chosen': 'Unsloth contributes to reducing memory usage during LLM fine-tuning by rewriting all Pytorch modules into Triton kernels, resulting in a 40% decrease in memory usage.',\n  'rejected': 'Unsloth increases memory usage during LLM fine-tuning.'},\n {'prompt': 'What is the primary goal of Unsloth for LLM fine-tuning?',\n  'chosen': 'The primary goal of Unsloth for LLM fine-tuning is to accelerate the process, achieving a 2x speedup while maintaining 0% accuracy degradation compared to normal QLoRA.',\n  'rejected': 'The primary goal of Unsloth for LLM fine-tuning is to slow down the process and increase memory usage.'},\n {'prompt': 'How does Unsloth improve LLM fine-tuning performance?',\n  'chosen': 'Unsloth improves LLM fine-tuning performance by manually deriving backpropagation steps and rewriting Pytorch modules into Triton kernels, resulting in a 2x speed increase and a 40% reduction in memory usage.',\n  'rejected': 'Unsloth degrades LLM fine-tuning performance compared to traditional methods.'},\n {'prompt': 'What makes Unsloth different from other tools for LLM fine-tuning?',\n  'chosen': 'What makes Unsloth different from other tools for LLM fine-tuning is its ability to achieve a 2x speed increase and a 40% reduction in memory usage while maintaining 0% accuracy degradation, thanks to optimized operations and Triton kernel rewriting.',\n  'rejected': 'Unsloth is not different from other tools for LLM fine-tuning.'},\n {'prompt': 'Why is Unsloth considered a lightweight library for LLM fine-tuning?',\n  'chosen': 'Unsloth is considered a lightweight library for LLM fine-tuning because it offers significant performance improvements, including a 2x speed increase and a 40% reduction in memory usage, without compromising accuracy.',\n  'rejected': 'Unsloth is considered a heavyweight library for LLM fine-tuning.'},\n {'prompt': 'What are the supported architectures for LLM fine-tuning by Unsloth?',\n  'chosen': 'Unsloth supports the Llama and Mistral architectures for LLM fine-tuning, ensuring compatibility with a range of model structures.',\n  'rejected': 'Unsloth does not support any specific architectures for LLM fine-tuning.'},\n {'prompt': 'How does Unsloth contribute to making LLM fine-tuning 2x faster?',\n  'chosen': 'Unsloth accelerates LLM fine-tuning by overwriting certain parts of the modeling code with optimized operations and manually deriving backpropagation steps, resulting in a 2x speedup without sacrificing accuracy.',\n  'rejected': 'Unsloth slows down LLM fine-tuning by introducing unnecessary complexities and approximations, leading to decreased efficiency.'},\n {'prompt': 'What benefits does Unsloth offer for LLM fine-tuning?',\n  'chosen': 'Unsloth provides a 2x acceleration in LLM fine-tuning speed, reduces memory usage by 40%, and ensures 0% accuracy degradation compared to regular QLoRA fine-tuning methods.',\n  'rejected': 'Unsloth has no impact on LLM fine-tuning efficiency and does not address memory usage concerns, leading to potential accuracy loss.'},\n {'prompt': 'Who developed the Unsloth library for LLM fine-tuning acceleration?',\n  'chosen': 'The Unsloth library was developed by Daniel, Michael, and the open-source community to address the need for faster fine-tuning of large language models.',\n  'rejected': 'Unsloth was solely developed by the Hugging Face team without any community contributions, making it less reliable and effective.'},\n {'prompt': 'Which GPUs are supported by Unsloth for LLM fine-tuning acceleration?',\n  'chosen': 'Unsloth supports most NVIDIA GPUs, ranging from GTX 1070 to H100s, ensuring compatibility with a wide range of hardware configurations for fine-tuning large language models.',\n  'rejected': 'Unsloth only supports outdated NVIDIA GPUs and is incompatible with modern hardware, limiting its usefulness for fine-tuning large language models.'},\n {'prompt': 'How does Unsloth ensure accuracy preservation during LLM fine-tuning?',\n  'chosen': 'Unsloth achieves 0% accuracy degradation by avoiding approximations in the optimized code and ensuring that all backpropagation steps are manually derived, maintaining fidelity to regular QLoRA fine-tuning methods.',\n  'rejected': 'Unsloth compromises accuracy during LLM fine-tuning by making approximations in the optimized code, resulting in a loss of model fidelity and decreased performance.'},\n {'prompt': 'What is the compatibility of Unsloth with the Hugging Face ecosystem?',\n  'chosen': 'Unsloth is fully compatible with the Hugging Face ecosystem, including Hub, transformers, PEFT, and TRL libraries, providing seamless integration into existing workflows for fine-tuning large language models.',\n  'rejected': 'Unsloth lacks compatibility with the Hugging Face ecosystem, requiring extensive modifications to existing workflows and libraries for integration, which can lead to inefficiencies and errors.'}]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data = data[:16]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-14T09:48:33.915596900Z",
     "start_time": "2024-08-14T09:48:33.887571800Z"
    }
   },
   "id": "c4947ba3cc9fb205"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2、数据集处理"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6eaa11e7c529604"
  },
  {
   "cell_type": "markdown",
   "source": [
    "了解DPO训练流程的可以知道，一般的DPO实现是需要将prompt(即instruction)分别和chsoen、rejected拼接在一起的。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4726f451d1b5a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4ce6798d1afd79ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOSS "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9777171077888583"
  },
  {
   "cell_type": "markdown",
   "source": [
    "DPO主要是两个模型，policy model(即我们主要要调优的模型) 和 reference model(用来约束的模型)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c97e1b5434c5aa01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class DPOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    DPO Loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beta: float=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        policy_chosen_logps: torch.Tensor,\n",
    "        policy_rejected_logps: torch.Tensor,\n",
    "        reference_chosen_logps: torch.Tensor,\n",
    "        reference_rejected_logps: torch.Tensor,\n",
    "    ) :\n",
    "        \"\"\"\n",
    "        policy_chosen_logps: 模型输出的对数概率。Shape: (batch_size,)\n",
    "        policy_rejected_logps:   Shape: (batch_size,)\n",
    "        reference_chosen_logps: Shape: (batch_size,)\n",
    "        reference_rejected_logps: Shape: (batch_size,)\n",
    "        \n",
    "        \"\"\"\n",
    "        policy_logps = policy_chosen_logps - policy_rejected_logps\n",
    "        reference_logps = reference_chosen_logps - reference_rejected_logps\n",
    "        logits = policy_logps - reference_logps\n",
    "        \n",
    "        loss = -F.logsigmoid(self.beta * logits)\n",
    "        \n",
    "        # 下面两个用于追踪训练的进度\n",
    "        chosen_rewards = (policy_chosen_logps - reference_chosen_logps).detach()\n",
    "        rejected_rewards = (policy_rejected_logps - reference_rejected_logps).detach()\n",
    "        \n",
    "        # 对每个batch进行平均\n",
    "        return loss.mean(), chosen_rewards.mean(), rejected_rewards.mean()\n",
    "\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c245fc84671838dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "计算log probs ,也就是 $\\pi_\\theta (y_w \\mid x)$,"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b1be59f347b82bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_logprobs(logits, labels, mask=None):\n",
    "    \"\"\"\n",
    "    logits:  shape (batch_size, sequence_len, vocab_size)\n",
    "    labels:  shape (batch_size, sequence_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 需要先进行位移操作\n",
    "    # 去掉标签的第一个\n",
    "    labels = labels[:, 1:].clone()\n",
    "    # 去掉模型输出的最后一个\n",
    "    logits = logits[:,:-1,:]\n",
    "    \n",
    "    logps = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    select_logprobs = torch.gather(\n",
    "        input=logps,\n",
    "        dim=1,\n",
    "        index=labels.unsqueeze(1)\n",
    "    ).squeeze(1)\n",
    "    \n",
    "    if mask is not None:\n",
    "        mask = mask[:,1:].clone()\n",
    "        # 进行掩码padding部分\n",
    "        select_logprobs = select_logprobs * mask\n",
    "        # 计算平均\n",
    "        average_logprobs = select_logprobs.sum(-1) / mask.sum(-1)\n",
    "        return average_logprobs\n",
    "    else:\n",
    "        return  select_logprobs.mean(-1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca63797467a68c1e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "clone 示例"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5636ff98ad0814b3"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.tensor([1,2,3])\n",
    "mask1 = mask\n",
    "mask1 += 1\n",
    "print(mask)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-12T16:24:01.170731600Z",
     "start_time": "2024-08-12T16:24:01.158727300Z"
    }
   },
   "id": "72b9adb35c2cf553"
  },
  {
   "cell_type": "markdown",
   "source": [
    "tensor shape示例"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbac1fe29cc58257"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4170],\n",
      "        [-2.4200]]) torch.Size([2, 1])\n",
      "tensor([-0.4170, -2.4200]) torch.Size([2])\n",
      "tensor(1.4185) tensor(1.4185)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "logits = torch.tensor(\n",
    "    [[2.0, 1.0, 0.1],\n",
    "     [0.5, 2.5, 0.3]])  # Shape: (2, 3)\n",
    "targets = torch.tensor([0, 2])  # Shape: (2,)\n",
    "# print(targets.unsqueeze(-1).shape)\n",
    "\n",
    "# Manual loss using torch.gather\n",
    "log_softmax_logits = F.log_softmax(logits, dim=1)  # Shape: (2, 3)\n",
    "# print(log_softmax_logits)\n",
    "selected_log_probs = torch.gather(\n",
    "    input=log_softmax_logits,\n",
    "    dim=1,\n",
    "    index=targets.unsqueeze(1), # Shape 2, 1\n",
    ") # Shape: (2,)\n",
    "print(selected_log_probs,selected_log_probs.shape)\n",
    "print(selected_log_probs.squeeze(1),selected_log_probs.squeeze(1).shape)\n",
    "manual_loss = -selected_log_probs.mean()  # Averaging over the batch\n",
    "\n",
    "\n",
    "# PyTorch loss\n",
    "cross_entropy_loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "print(manual_loss, cross_entropy_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-12T16:01:46.797611600Z",
     "start_time": "2024-08-12T16:01:44.279995600Z"
    }
   },
   "id": "4b1f1f33b9e7f613"
  },
  {
   "cell_type": "markdown",
   "source": [
    "进行batch的dpo loss计算"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4043bc53c7bcd4d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_batch_loss(batch, policy_model, reference_model, beta):\n",
    "    \"\"\"Compute the DPO loss on an input batch\"\"\"\n",
    "    loss_fn = DPOLoss(beta)\n",
    "    \n",
    "    policy_chosen_logps = compute_logprobs(\n",
    "        logits=policy_model(batch[\"chosen\"]),\n",
    "        labels=batch[\"chosen\"],\n",
    "        mask=batch[\"chosen_mask\"]\n",
    "    )\n",
    "    policy_rejected_logps = compute_logprobs(\n",
    "        logits=policy_model(batch[\"rejected\"]),\n",
    "        labels=batch[\"rejected\"],\n",
    "        mask=batch[\"rejected_mask\"]\n",
    "    )\n",
    "    reference_chosen_logps = compute_logprobs(\n",
    "        logits=reference_model(batch['chosen']),\n",
    "        labels=batch['chosen'],\n",
    "        mask=batch[\"chosen_mask\"]\n",
    "    )\n",
    "    reference_rejected_logps = compute_logprobs(\n",
    "        logits=reference_model(batch['rejected']),\n",
    "        labels=batch['rejected'],\n",
    "        mask=batch[\"rejected_mask\"]\n",
    "    )\n",
    "    loss, chosen_rewards, rejected_rewards = loss_fn(\n",
    "        policy_chosen_logps=policy_chosen_logps,\n",
    "        policy_rejected_logps=policy_rejected_logps,\n",
    "        reference_chosen_logps=reference_chosen_logps,\n",
    "        reference_rejected_logps=reference_rejected_logps,\n",
    "        beta=beta\n",
    "    )\n",
    "    return loss, chosen_rewards, rejected_rewards"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3211a04a645fb478"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "89e5c1930fb1041"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
